{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c5976e",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "# Student Name: Hung Yi-Le\n",
    "\n",
    "# GITHUB: ZackLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67c5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386518ec",
   "metadata": {},
   "source": [
    "# 1. Text Classification\n",
    "## It is highly recommended that you complete this project using Keras1 and Python.\n",
    "# (a) In this problem, we are trying to build a classifier to analyze the sentiment of reviews. You are provided with text data in two folders: one folder involves positive reviews, and one folder involves negative reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99c45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "from string import digits\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98fec32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0d625",
   "metadata": {},
   "source": [
    "# (b) Data Exploration and Pre-processing\n",
    "\n",
    "\n",
    "## i. You can use binary encoding for the sentiments , i.e y = 1 for positive sentiments and y = −1 for negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11221a",
   "metadata": {},
   "source": [
    "## ii. The data are pretty clean. Remove the punctuation and numbers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ac68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = {}\n",
    "negative = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b187cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_by_value(d, reverse = False):\n",
    "  return dict(sorted(d.items(), key = lambda x: x[1], reverse = reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16ebbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../data/pos')\n",
    "pos_files = glob.glob('*.txt')\n",
    "#sort all the txt file from 001 up to certain number in order\n",
    "pos_files = sorted(pos_files, key=lambda name: int(name[2:5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da81dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specific_word = {}\n",
    "count = []\n",
    "len_word = []\n",
    "tokenized = {}\n",
    "dd = {}\n",
    "for file in pos_files:\n",
    "    with open(file) as f:\n",
    "        d = {}\n",
    "        \n",
    "        #read the whole txt file content\n",
    "        lines = f.readlines()\n",
    "        #remove all the punctuation\n",
    "        lines = [''.join(letter for letter in line if letter not in string.punctuation) for line in lines]\n",
    "        #remove all the numbers\n",
    "        lines = [''.join(letter for letter in line if letter not in string.digits) for line in lines]\n",
    "        data = []\n",
    "        data2 = []\n",
    "        #remove all the \\n\n",
    "        for x in lines:\n",
    "            data2.append(x.replace(\"\\n\", \"\"))\n",
    "        data = \"\".join(data2)\n",
    "        \n",
    "        #assign in a dic\n",
    "        positive[file] = data\n",
    "        \n",
    "        #append the length of the words in a list\n",
    "        len_word.append(len(data.split()))\n",
    "        \n",
    "        #read all the unique word in the whole txt file\n",
    "        words = set(data.split())\n",
    "        #count all the unique word in the list\n",
    "        count.append(len(words))\n",
    "        \n",
    "        data1 = data.split()\n",
    "        \n",
    "#         t.fit_on_texts(data2)\n",
    "#         tokenized[file] = t.texts_to_sequences(data2)\n",
    "#         dd[file] = t.word_index\n",
    "        \n",
    "        \n",
    "        #make all the word into lowercase\n",
    "        data1 = [x.lower() for x in data1]\n",
    "        for word in data1:\n",
    "            if word in d:\n",
    "                d[word] = d[word] + 1\n",
    "            else:\n",
    "                d[word] = 1\n",
    "        d = sort_dict_by_value(d, True)\n",
    "        #for specific word counted\n",
    "        df1_1 = pd.Series(d.values(), index = d.keys())\n",
    "        df1_1 = pd.DataFrame(df1_1)\n",
    "        df_specific_word[file] = df1_1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2e9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(positive.values(), index = positive.keys())\n",
    "df = pd.DataFrame(df)\n",
    "df['class'] = '1'\n",
    "df['number of unique words'] = count\n",
    "df['review_length'] = len_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d189241b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "      <th>number of unique words</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv000_29590.txt</th>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv001_18431.txt</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv002_15918.txt</th>\n",
       "      <td>youve got mail works alot better than it deser...</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv003_11664.txt</th>\n",
       "      <td>jaws  is a rare film that grabs your attenti...</td>\n",
       "      <td>1</td>\n",
       "      <td>496</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv004_11636.txt</th>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv995_21821.txt</th>\n",
       "      <td>wow  what a movie  its everything a movie can ...</td>\n",
       "      <td>1</td>\n",
       "      <td>314</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv996_11592.txt</th>\n",
       "      <td>richard gere can be a commanding actor  but he...</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv997_5046.txt</th>\n",
       "      <td>glorystarring matthew broderick  denzel washin...</td>\n",
       "      <td>1</td>\n",
       "      <td>510</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv998_14111.txt</th>\n",
       "      <td>steven spielbergs second epic film on world wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>269</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv999_13106.txt</th>\n",
       "      <td>truman   trueman   burbank is the perfect name...</td>\n",
       "      <td>1</td>\n",
       "      <td>447</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0 class  \\\n",
       "cv000_29590.txt  films adapted from comic books have had plenty...     1   \n",
       "cv001_18431.txt  every now and then a movie comes along from a ...     1   \n",
       "cv002_15918.txt  youve got mail works alot better than it deser...     1   \n",
       "cv003_11664.txt    jaws  is a rare film that grabs your attenti...     1   \n",
       "cv004_11636.txt  moviemaking is a lot like being the general ma...     1   \n",
       "...                                                            ...   ...   \n",
       "cv995_21821.txt  wow  what a movie  its everything a movie can ...     1   \n",
       "cv996_11592.txt  richard gere can be a commanding actor  but he...     1   \n",
       "cv997_5046.txt   glorystarring matthew broderick  denzel washin...     1   \n",
       "cv998_14111.txt  steven spielbergs second epic film on world wa...     1   \n",
       "cv999_13106.txt  truman   trueman   burbank is the perfect name...     1   \n",
       "\n",
       "                 number of unique words  review_length  \n",
       "cv000_29590.txt                     411            679  \n",
       "cv001_18431.txt                     311            650  \n",
       "cv002_15918.txt                     243            416  \n",
       "cv003_11664.txt                     496            997  \n",
       "cv004_11636.txt                     341            644  \n",
       "...                                 ...            ...  \n",
       "cv995_21821.txt                     314            760  \n",
       "cv996_11592.txt                     196            320  \n",
       "cv997_5046.txt                      510           1050  \n",
       "cv998_14111.txt                     269            583  \n",
       "cv999_13106.txt                     447            986  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cc32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../neg')\n",
    "neg_files = glob.glob('*.txt')\n",
    "#sort all the txt file from 001 up to certain number in order\n",
    "neg_files = sorted(neg_files, key=lambda name: int(name[2:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "874bf07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/charlieblah/Desktop/School/Spring 2022/DSCI552/HW/Project/data/neg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get the current working directory\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1950df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specific_word1 = {}\n",
    "count1 = []\n",
    "len_word1 = []\n",
    "tokenized1 = {}\n",
    "dd1 = {}\n",
    "for file in neg_files:\n",
    "    with open(file) as f:\n",
    "        d1 = {}\n",
    "        #read the whole txt file content\n",
    "        lines = f.readlines()\n",
    "    \n",
    "        #remove all the punctuation\n",
    "        lines = [''.join(letter for letter in line if letter not in string.punctuation) for line in lines]\n",
    "        #remove all the numbers\n",
    "        lines = [''.join(letter for letter in line if letter not in string.digits) for line in lines]\n",
    "        data = []\n",
    "        data2 = []\n",
    "        \n",
    "        for  x in lines:\n",
    "            data2.append(x.replace(\"\\n\", \"\"))\n",
    "        data = \"\".join(data2)\n",
    "        #assign in a dic\n",
    "        negative[file] = data\n",
    "        #append the length of the words in a list\n",
    "        len_word1.append(len(data.split()))\n",
    "        \n",
    "        #read all the unique word in the whole txt file\n",
    "        words = set(data.split())\n",
    "        #count all hte unique word in the list\n",
    "        count1.append(len(words))\n",
    "        \n",
    "        data1 = data.split()\n",
    "        \n",
    "#         t.fit_on_texts(data2)\n",
    "#         tokenized1[file] = t.texts_to_sequences(data2)\n",
    "#         dd1[file] = t.word_index\n",
    "        \n",
    "        #make all the word into lowercase\n",
    "        data1 = [x.lower() for x in data1]\n",
    "        for word in data1:\n",
    "            if word in d1:\n",
    "                d1[word] = d1[word] + 1\n",
    "            else:\n",
    "                d1[word] = 1\n",
    "        d1 = sort_dict_by_value(d1, True)\n",
    "        #for specific word counted\n",
    "        df1_1 = pd.Series(d1.values(), index = d1.keys())\n",
    "        df1_1 = pd.DataFrame(df1_1)\n",
    "        df_specific_word1[file] = df1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984006a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0e576d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.Series(negative.values(), index = negative.keys())\n",
    "df1 = pd.DataFrame(df1)\n",
    "df1['class'] = '0'\n",
    "df1['number of unique words'] = count1\n",
    "df1['review_length'] = len_word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34349803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "      <th>number of unique words</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv000_29416.txt</th>\n",
       "      <td>plot  two teen couples go to a church party  d...</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv001_19502.txt</th>\n",
       "      <td>the happy bastards quick movie review damn tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv002_17424.txt</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv003_12683.txt</th>\n",
       "      <td>quest for camelot  is warner bros   first fe...</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv004_12641.txt</th>\n",
       "      <td>synopsis  a mentally unstable man undergoing p...</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv995_23113.txt</th>\n",
       "      <td>if anything   stigmata  should be taken as a w...</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>1366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv996_12447.txt</th>\n",
       "      <td>john boormans  zardoz  is a goofy cinematic de...</td>\n",
       "      <td>0</td>\n",
       "      <td>507</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv997_5152.txt</th>\n",
       "      <td>the kids in the hall are an acquired taste  it...</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv998_15691.txt</th>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv999_14636.txt</th>\n",
       "      <td>two party guys bob their heads to haddaways da...</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0 class  \\\n",
       "cv000_29416.txt  plot  two teen couples go to a church party  d...     0   \n",
       "cv001_19502.txt  the happy bastards quick movie review damn tha...     0   \n",
       "cv002_17424.txt  it is movies like these that make a jaded movi...     0   \n",
       "cv003_12683.txt    quest for camelot  is warner bros   first fe...     0   \n",
       "cv004_12641.txt  synopsis  a mentally unstable man undergoing p...     0   \n",
       "...                                                            ...   ...   \n",
       "cv995_23113.txt  if anything   stigmata  should be taken as a w...     0   \n",
       "cv996_12447.txt  john boormans  zardoz  is a goofy cinematic de...     0   \n",
       "cv997_5152.txt   the kids in the hall are an acquired taste  it...     0   \n",
       "cv998_15691.txt  there was a time when john carpenter was a gre...     0   \n",
       "cv999_14636.txt  two party guys bob their heads to haddaways da...     0   \n",
       "\n",
       "                 number of unique words  review_length  \n",
       "cv000_29416.txt                     332            688  \n",
       "cv001_19502.txt                     151            240  \n",
       "cv002_17424.txt                     268            483  \n",
       "cv003_12683.txt                     303            462  \n",
       "cv004_12641.txt                     371            718  \n",
       "...                                 ...            ...  \n",
       "cv995_23113.txt                     575           1366  \n",
       "cv996_12447.txt                     507           1009  \n",
       "cv997_5152.txt                      236            396  \n",
       "cv998_15691.txt                     281            521  \n",
       "cv999_14636.txt                     291            503  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dce94c",
   "metadata": {},
   "source": [
    "## iii. The name of each text file starts with cv number. Use text files 0-699 in each class for training and 700-999 for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107be9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train = pd.concat([train, df[0:700]])\n",
    "train = pd.concat([train, df1[0:700]])\n",
    "test = pd.DataFrame()\n",
    "test = pd.concat([test, df[700:1000]])\n",
    "test = pd.concat([test, df1[700:1000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373f93d",
   "metadata": {},
   "source": [
    "### Train and test data are seperated as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2735679",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"\"\n",
    "for i in train[0]:\n",
    "    texts = texts + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15ad85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts([texts])\n",
    "tokenized = t.texts_to_sequences([texts])\n",
    "tokenized_P = t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8e60f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../pos')\n",
    "neg_files = glob.glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ae1d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized1 = {}\n",
    "dd = {}\n",
    "for file in pos_files:\n",
    "    with open(file) as f:\n",
    "        d = {}\n",
    "        \n",
    "        #read the whole txt file content\n",
    "        lines = f.readlines()\n",
    "        #remove all the punctuation\n",
    "        lines = [''.join(letter for letter in line if letter not in string.punctuation) for line in lines]\n",
    "        #remove all the numbers\n",
    "        lines = [''.join(letter for letter in line if letter not in string.digits) for line in lines]\n",
    "        data = []\n",
    "        data2 = []\n",
    "        #remove all the \\n\n",
    "        for x in lines:\n",
    "            data2.append(x.replace(\"\\n\", \"\"))\n",
    "            \n",
    "        tokenized1[file] = t.texts_to_sequences(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acdaf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(tokenized1.values(), index = positive.keys())\n",
    "df = pd.DataFrame(df)\n",
    "df['class'] = '1'\n",
    "df['number of unique words'] = count\n",
    "df['review_length'] = len_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573b996d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "      <th>number of unique words</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv000_29590.txt</th>\n",
       "      <td>[[66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 61...</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv001_18431.txt</th>\n",
       "      <td>[[149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 157...</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv002_15918.txt</th>\n",
       "      <td>[[715, 287, 4181, 376, 10591, 155, 59, 9, 1168...</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv003_11664.txt</th>\n",
       "      <td>[[2677, 6, 2, 1520, 15, 8, 5604, 164, 565, 145...</td>\n",
       "      <td>1</td>\n",
       "      <td>496</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv004_11636.txt</th>\n",
       "      <td>[[5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv995_21821.txt</th>\n",
       "      <td>[[3582, 47, 2, 28], [22, 289, 2, 28, 64, 23, 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>314</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv996_11592.txt</th>\n",
       "      <td>[[871, 4725, 64, 23, 2, 3871, 265, 16, 122, 26...</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv997_5046.txt</th>\n",
       "      <td>[[1294, 2924, 4064, 1841, 3, 3600, 1, 367, 67,...</td>\n",
       "      <td>1</td>\n",
       "      <td>510</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv998_14111.txt</th>\n",
       "      <td>[[1044, 4564, 339, 1470, 15, 19, 143, 381, 944...</td>\n",
       "      <td>1</td>\n",
       "      <td>269</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv999_13106.txt</th>\n",
       "      <td>[[1102, 6850, 6, 1, 402, 362, 12, 775, 3877, 7...</td>\n",
       "      <td>1</td>\n",
       "      <td>447</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0 class  \\\n",
       "cv000_29590.txt  [[66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 61...     1   \n",
       "cv001_18431.txt  [[149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 157...     1   \n",
       "cv002_15918.txt  [[715, 287, 4181, 376, 10591, 155, 59, 9, 1168...     1   \n",
       "cv003_11664.txt  [[2677, 6, 2, 1520, 15, 8, 5604, 164, 565, 145...     1   \n",
       "cv004_11636.txt  [[5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 2...     1   \n",
       "...                                                            ...   ...   \n",
       "cv995_21821.txt  [[3582, 47, 2, 28], [22, 289, 2, 28, 64, 23, 1...     1   \n",
       "cv996_11592.txt  [[871, 4725, 64, 23, 2, 3871, 265, 16, 122, 26...     1   \n",
       "cv997_5046.txt   [[1294, 2924, 4064, 1841, 3, 3600, 1, 367, 67,...     1   \n",
       "cv998_14111.txt  [[1044, 4564, 339, 1470, 15, 19, 143, 381, 944...     1   \n",
       "cv999_13106.txt  [[1102, 6850, 6, 1, 402, 362, 12, 775, 3877, 7...     1   \n",
       "\n",
       "                 number of unique words  review_length  \n",
       "cv000_29590.txt                     411            679  \n",
       "cv001_18431.txt                     311            650  \n",
       "cv002_15918.txt                     243            416  \n",
       "cv003_11664.txt                     496            997  \n",
       "cv004_11636.txt                     341            644  \n",
       "...                                 ...            ...  \n",
       "cv995_21821.txt                     314            760  \n",
       "cv996_11592.txt                     196            320  \n",
       "cv997_5046.txt                      510           1050  \n",
       "cv998_14111.txt                     269            583  \n",
       "cv999_13106.txt                     447            986  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bddd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../neg')\n",
    "neg_files = glob.glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fa45b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized2 = {}\n",
    "dd = {}\n",
    "for file in neg_files:\n",
    "    with open(file) as f:\n",
    "        d = {}\n",
    "        \n",
    "        #read the whole txt file content\n",
    "        lines = f.readlines()\n",
    "        #remove all the punctuation\n",
    "        lines = [''.join(letter for letter in line if letter not in string.punctuation) for line in lines]\n",
    "        #remove all the numbers\n",
    "        lines = [''.join(letter for letter in line if letter not in string.digits) for line in lines]\n",
    "        data = []\n",
    "        data2 = []\n",
    "        #remove all the \\n\n",
    "        for x in lines:\n",
    "            data2.append(x.replace(\"\\n\", \"\"))\n",
    "            \n",
    "        tokenized2[file] = t.texts_to_sequences(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f166969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.Series(tokenized2.values(), index = negative.keys())\n",
    "df1 = pd.DataFrame(df1)\n",
    "df1['class'] = '0'\n",
    "df1['number of unique words'] = count1\n",
    "df1['review_length'] = len_word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66d732d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "      <th>number of unique words</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv000_29416.txt</th>\n",
       "      <td>[[104, 104], [104], [8, 25, 651, 132, 5, 262, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv001_19502.txt</th>\n",
       "      <td>[[170, 9, 1, 1784, 1656, 4, 2, 107, 1111, 1052...</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv002_17424.txt</th>\n",
       "      <td>[[6776, 6, 26, 2, 28, 9, 6, 2, 33601, 15574, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv003_12683.txt</th>\n",
       "      <td>[[22648, 1, 1722, 340], [119, 1789, 1, 4974], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv004_12641.txt</th>\n",
       "      <td>[[43, 518, 4700, 368, 218, 1053, 211, 82, 3038...</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv995_23113.txt</th>\n",
       "      <td>[[1779, 43, 2, 11995, 33645, 7, 1, 4812, 2215,...</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>1366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv996_12447.txt</th>\n",
       "      <td>[[22, 141, 1, 7222, 4, 1, 32790, 4, 1323, 297,...</td>\n",
       "      <td>0</td>\n",
       "      <td>507</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv997_5152.txt</th>\n",
       "      <td>[[32769, 11, 1, 14410, 3603, 4, 107, 8, 628, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv998_15691.txt</th>\n",
       "      <td>[[3, 141, 1, 15910, 1289, 1324, 424, 4, 1242, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv999_14636.txt</th>\n",
       "      <td>[[3538, 192, 455, 3, 50, 1687, 524], [3538, 52...</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0 class  \\\n",
       "cv000_29416.txt  [[104, 104], [104], [8, 25, 651, 132, 5, 262, ...     0   \n",
       "cv001_19502.txt  [[170, 9, 1, 1784, 1656, 4, 2, 107, 1111, 1052...     0   \n",
       "cv002_17424.txt  [[6776, 6, 26, 2, 28, 9, 6, 2, 33601, 15574, 1...     0   \n",
       "cv003_12683.txt  [[22648, 1, 1722, 340], [119, 1789, 1, 4974], ...     0   \n",
       "cv004_12641.txt  [[43, 518, 4700, 368, 218, 1053, 211, 82, 3038...     0   \n",
       "...                                                            ...   ...   \n",
       "cv995_23113.txt  [[1779, 43, 2, 11995, 33645, 7, 1, 4812, 2215,...     0   \n",
       "cv996_12447.txt  [[22, 141, 1, 7222, 4, 1, 32790, 4, 1323, 297,...     0   \n",
       "cv997_5152.txt   [[32769, 11, 1, 14410, 3603, 4, 107, 8, 628, 1...     0   \n",
       "cv998_15691.txt  [[3, 141, 1, 15910, 1289, 1324, 424, 4, 1242, ...     0   \n",
       "cv999_14636.txt  [[3538, 192, 455, 3, 50, 1687, 524], [3538, 52...     0   \n",
       "\n",
       "                 number of unique words  review_length  \n",
       "cv000_29416.txt                     332            688  \n",
       "cv001_19502.txt                     151            240  \n",
       "cv002_17424.txt                     268            483  \n",
       "cv003_12683.txt                     303            462  \n",
       "cv004_12641.txt                     371            718  \n",
       "...                                 ...            ...  \n",
       "cv995_23113.txt                     575           1366  \n",
       "cv996_12447.txt                     507           1009  \n",
       "cv997_5152.txt                      236            396  \n",
       "cv998_15691.txt                     281            521  \n",
       "cv999_14636.txt                     291            503  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "286aed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df,df1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f5031",
   "metadata": {},
   "source": [
    "## iv. Count the number of unique words in the whole dataset (train + test) and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10c32bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "\n",
    "for file_name in df_specific_word:\n",
    "    current_file_words = df_specific_word[file_name].to_dict()[0]\n",
    "    for word in current_file_words:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] = int(current_file_words[word])\n",
    "        else:\n",
    "            word_counts[word] += int(current_file_words[word])\n",
    "    \n",
    "for file_name in df_specific_word1:\n",
    "    current_file_words = df_specific_word1[file_name].to_dict()[0]\n",
    "    for word in current_file_words:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] = int(current_file_words[word])\n",
    "        else:\n",
    "            word_counts[word] += int(current_file_words[word])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "961b05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.Series(word_counts.values(), index = word_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5507d820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the                 76327\n",
       "and                 35351\n",
       "in                  21598\n",
       "a                   37964\n",
       "to                  31763\n",
       "                    ...  \n",
       "stonily                 1\n",
       "jokea                   1\n",
       "jerrymaguirewill        1\n",
       "jerrymaguire            1\n",
       "roxburys                1\n",
       "Length: 46830, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7905cac",
   "metadata": {},
   "source": [
    "### The number of unique words in the whole dataset is shown as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4933d3b",
   "metadata": {},
   "source": [
    "## v. Calculate the average review length and the standard deviation of review lengths. Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b6919da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644.3555"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['review_length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "039a0308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285.0511431249635"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['review_length'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032544c",
   "metadata": {},
   "source": [
    "### Mean of review length is about 644.3555, and standard deviation is about 285.0511431249635"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2551f",
   "metadata": {},
   "source": [
    "## vi. Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46843e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='review_length', ylabel='Count'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data = df2, x = df2['review_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336b112",
   "metadata": {},
   "source": [
    "### The histogram of review lengths is shown as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72fbbf",
   "metadata": {},
   "source": [
    "## vii. To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to represent text using popularity/ rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f63416e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "      <th>number of unique words</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv000_29590.txt</th>\n",
       "      <td>[[66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 61...</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv001_18431.txt</th>\n",
       "      <td>[[149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 157...</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv002_15918.txt</th>\n",
       "      <td>[[715, 287, 4181, 376, 10591, 155, 59, 9, 1168...</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv003_11664.txt</th>\n",
       "      <td>[[2677, 6, 2, 1520, 15, 8, 5604, 164, 565, 145...</td>\n",
       "      <td>1</td>\n",
       "      <td>496</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv004_11636.txt</th>\n",
       "      <td>[[5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv995_23113.txt</th>\n",
       "      <td>[[1779, 43, 2, 11995, 33645, 7, 1, 4812, 2215,...</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>1366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv996_12447.txt</th>\n",
       "      <td>[[22, 141, 1, 7222, 4, 1, 32790, 4, 1323, 297,...</td>\n",
       "      <td>0</td>\n",
       "      <td>507</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv997_5152.txt</th>\n",
       "      <td>[[32769, 11, 1, 14410, 3603, 4, 107, 8, 628, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv998_15691.txt</th>\n",
       "      <td>[[3, 141, 1, 15910, 1289, 1324, 424, 4, 1242, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv999_14636.txt</th>\n",
       "      <td>[[3538, 192, 455, 3, 50, 1687, 524], [3538, 52...</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0 class  \\\n",
       "cv000_29590.txt  [[66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 61...     1   \n",
       "cv001_18431.txt  [[149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 157...     1   \n",
       "cv002_15918.txt  [[715, 287, 4181, 376, 10591, 155, 59, 9, 1168...     1   \n",
       "cv003_11664.txt  [[2677, 6, 2, 1520, 15, 8, 5604, 164, 565, 145...     1   \n",
       "cv004_11636.txt  [[5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 2...     1   \n",
       "...                                                            ...   ...   \n",
       "cv995_23113.txt  [[1779, 43, 2, 11995, 33645, 7, 1, 4812, 2215,...     0   \n",
       "cv996_12447.txt  [[22, 141, 1, 7222, 4, 1, 32790, 4, 1323, 297,...     0   \n",
       "cv997_5152.txt   [[32769, 11, 1, 14410, 3603, 4, 107, 8, 628, 1...     0   \n",
       "cv998_15691.txt  [[3, 141, 1, 15910, 1289, 1324, 424, 4, 1242, ...     0   \n",
       "cv999_14636.txt  [[3538, 192, 455, 3, 50, 1687, 524], [3538, 52...     0   \n",
       "\n",
       "                 number of unique words  review_length  \n",
       "cv000_29590.txt                     411            679  \n",
       "cv001_18431.txt                     311            650  \n",
       "cv002_15918.txt                     243            416  \n",
       "cv003_11664.txt                     496            997  \n",
       "cv004_11636.txt                     341            644  \n",
       "...                                 ...            ...  \n",
       "cv995_23113.txt                     575           1366  \n",
       "cv996_12447.txt                     507           1009  \n",
       "cv997_5152.txt                      236            396  \n",
       "cv998_15691.txt                     281            521  \n",
       "cv999_14636.txt                     291            503  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a2fab",
   "metadata": {},
   "source": [
    "### The output result shown as above are the words common use for each txt files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d8d56",
   "metadata": {},
   "source": [
    "### The output result shown as above are the tokenize result for each files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785bbba",
   "metadata": {},
   "source": [
    "## viii. Select a review length L that 70% of the reviews have a length below it. If you feel more adventurous, set the threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fef6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d4e3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame()\n",
    "df3 = pd.concat([df,df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22ae1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.percentile(df3['review_length'].to_numpy(),70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8a91553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205107b5",
   "metadata": {},
   "source": [
    "### The length of L is 737"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd5e57",
   "metadata": {},
   "source": [
    "## ix. Truncate reviews longer than L words and zeropad reviews shorter than L so that all texts (= data points) are of length L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9263c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-ebdbb47cba09>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3[0][i] = [x for l in df3[0][i] for x in l]\n"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(df3.index):\n",
    "    df3[0][i] = [x for l in df3[0][i] for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5951b48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "      <th>number of unique words</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv000_29590.txt</th>\n",
       "      <td>[66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 616...</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv001_18431.txt</th>\n",
       "      <td>[149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 1574...</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv002_15918.txt</th>\n",
       "      <td>[715, 287, 4181, 376, 10591, 155, 59, 9, 1168,...</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv003_11664.txt</th>\n",
       "      <td>[2677, 6, 2, 1520, 15, 8, 5604, 164, 565, 145,...</td>\n",
       "      <td>1</td>\n",
       "      <td>496</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv004_11636.txt</th>\n",
       "      <td>[5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 24...</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv995_23113.txt</th>\n",
       "      <td>[1779, 43, 2, 11995, 33645, 7, 1, 4812, 2215, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>1366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv996_12447.txt</th>\n",
       "      <td>[22, 141, 1, 7222, 4, 1, 32790, 4, 1323, 297, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>507</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv997_5152.txt</th>\n",
       "      <td>[32769, 11, 1, 14410, 3603, 4, 107, 8, 628, 11...</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv998_15691.txt</th>\n",
       "      <td>[3, 141, 1, 15910, 1289, 1324, 424, 4, 1242, 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv999_14636.txt</th>\n",
       "      <td>[3538, 192, 455, 3, 50, 1687, 524, 3538, 523, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0 class  \\\n",
       "cv000_29590.txt  [66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 616...     1   \n",
       "cv001_18431.txt  [149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 1574...     1   \n",
       "cv002_15918.txt  [715, 287, 4181, 376, 10591, 155, 59, 9, 1168,...     1   \n",
       "cv003_11664.txt  [2677, 6, 2, 1520, 15, 8, 5604, 164, 565, 145,...     1   \n",
       "cv004_11636.txt  [5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 24...     1   \n",
       "...                                                            ...   ...   \n",
       "cv995_23113.txt  [1779, 43, 2, 11995, 33645, 7, 1, 4812, 2215, ...     0   \n",
       "cv996_12447.txt  [22, 141, 1, 7222, 4, 1, 32790, 4, 1323, 297, ...     0   \n",
       "cv997_5152.txt   [32769, 11, 1, 14410, 3603, 4, 107, 8, 628, 11...     0   \n",
       "cv998_15691.txt  [3, 141, 1, 15910, 1289, 1324, 424, 4, 1242, 3...     0   \n",
       "cv999_14636.txt  [3538, 192, 455, 3, 50, 1687, 524, 3538, 523, ...     0   \n",
       "\n",
       "                 number of unique words  review_length  \n",
       "cv000_29590.txt                     411            679  \n",
       "cv001_18431.txt                     311            650  \n",
       "cv002_15918.txt                     243            416  \n",
       "cv003_11664.txt                     496            997  \n",
       "cv004_11636.txt                     341            644  \n",
       "...                                 ...            ...  \n",
       "cv995_23113.txt                     575           1366  \n",
       "cv996_12447.txt                     507           1009  \n",
       "cv997_5152.txt                      236            396  \n",
       "cv998_15691.txt                     281            521  \n",
       "cv999_14636.txt                     291            503  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cbf9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "077c44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pad_sequences(df4[0], maxlen = int(L), padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7ff20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-7ab0f01bea86>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4[0][i] = tt[i]\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(0,len(df4),1)):\n",
    "    df4[0][i] = tt[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf101a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "      <th>number of unique words</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv000_29590.txt</th>\n",
       "      <td>[66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 616...</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv001_18431.txt</th>\n",
       "      <td>[149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 1574...</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv002_15918.txt</th>\n",
       "      <td>[715, 287, 4181, 376, 10591, 155, 59, 9, 1168,...</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv003_11664.txt</th>\n",
       "      <td>[1418, 4372, 2476, 8615, 115, 261, 5, 2604, 16...</td>\n",
       "      <td>1</td>\n",
       "      <td>496</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv004_11636.txt</th>\n",
       "      <td>[5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 24...</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv995_23113.txt</th>\n",
       "      <td>[1779, 43, 2, 11995, 33645, 7, 1, 4812, 2215, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>1366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv996_12447.txt</th>\n",
       "      <td>[22, 141, 1, 7222, 4, 1, 32790, 4, 1323, 297, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>507</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv997_5152.txt</th>\n",
       "      <td>[32769, 11, 1, 14410, 3603, 4, 107, 8, 628, 11...</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv998_15691.txt</th>\n",
       "      <td>[3, 141, 1, 15910, 1289, 1324, 424, 4, 1242, 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv999_14636.txt</th>\n",
       "      <td>[3538, 192, 455, 3, 50, 1687, 524, 3538, 523, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0 class  \\\n",
       "cv000_29590.txt  [66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 616...     1   \n",
       "cv001_18431.txt  [149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 1574...     1   \n",
       "cv002_15918.txt  [715, 287, 4181, 376, 10591, 155, 59, 9, 1168,...     1   \n",
       "cv003_11664.txt  [1418, 4372, 2476, 8615, 115, 261, 5, 2604, 16...     1   \n",
       "cv004_11636.txt  [5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 24...     1   \n",
       "...                                                            ...   ...   \n",
       "cv995_23113.txt  [1779, 43, 2, 11995, 33645, 7, 1, 4812, 2215, ...     0   \n",
       "cv996_12447.txt  [22, 141, 1, 7222, 4, 1, 32790, 4, 1323, 297, ...     0   \n",
       "cv997_5152.txt   [32769, 11, 1, 14410, 3603, 4, 107, 8, 628, 11...     0   \n",
       "cv998_15691.txt  [3, 141, 1, 15910, 1289, 1324, 424, 4, 1242, 3...     0   \n",
       "cv999_14636.txt  [3538, 192, 455, 3, 50, 1687, 524, 3538, 523, ...     0   \n",
       "\n",
       "                 number of unique words  review_length  \n",
       "cv000_29590.txt                     411            679  \n",
       "cv001_18431.txt                     311            650  \n",
       "cv002_15918.txt                     243            416  \n",
       "cv003_11664.txt                     496            997  \n",
       "cv004_11636.txt                     341            644  \n",
       "...                                 ...            ...  \n",
       "cv995_23113.txt                     575           1366  \n",
       "cv996_12447.txt                     507           1009  \n",
       "cv997_5152.txt                      236            396  \n",
       "cv998_15691.txt                     281            521  \n",
       "cv999_14636.txt                     291            503  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75cd44",
   "metadata": {},
   "source": [
    "### The truncate result is shown as above where I put them in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3fb7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train = pd.concat([train, df4[0:700]])\n",
    "train = pd.concat([train, df4[1000:1700]])\n",
    "test = pd.DataFrame()\n",
    "test = pd.concat([test, df4[700:1000]])\n",
    "test = pd.concat([test, df4[1700:2000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca09b982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "      <th>number of unique words</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv000_29590.txt</th>\n",
       "      <td>[66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 616...</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv001_18431.txt</th>\n",
       "      <td>[149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 1574...</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv002_15918.txt</th>\n",
       "      <td>[715, 287, 4181, 376, 10591, 155, 59, 9, 1168,...</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv003_11664.txt</th>\n",
       "      <td>[1418, 4372, 2476, 8615, 115, 261, 5, 2604, 16...</td>\n",
       "      <td>1</td>\n",
       "      <td>496</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv004_11636.txt</th>\n",
       "      <td>[5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 24...</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv695_22268.txt</th>\n",
       "      <td>[21603, 526, 1, 1477, 768, 24, 1636, 198, 186,...</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv696_29619.txt</th>\n",
       "      <td>[1823, 4, 7043, 1201, 18, 32, 151, 12, 1, 15, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>470</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv697_12106.txt</th>\n",
       "      <td>[384, 21, 387, 1028, 38622, 3, 196, 9049, 353,...</td>\n",
       "      <td>0</td>\n",
       "      <td>355</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv698_16930.txt</th>\n",
       "      <td>[25, 4, 106, 617, 894, 107, 6, 2451, 1155, 393...</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv699_7773.txt</th>\n",
       "      <td>[1, 646, 4, 14, 28, 6, 83, 262, 10825, 536, 84...</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0 class  \\\n",
       "cv000_29590.txt  [66, 3436, 31, 344, 1573, 32, 90, 1069, 4, 616...     1   \n",
       "cv001_18431.txt  [149, 141, 3, 97, 2, 28, 200, 256, 31, 2, 1574...     1   \n",
       "cv002_15918.txt  [715, 287, 4181, 376, 10591, 155, 59, 9, 1168,...     1   \n",
       "cv003_11664.txt  [1418, 4372, 2476, 8615, 115, 261, 5, 2604, 16...     1   \n",
       "cv004_11636.txt  [5346, 6, 2, 207, 38, 112, 1, 835, 2680, 4, 24...     1   \n",
       "...                                                            ...   ...   \n",
       "cv695_22268.txt  [21603, 526, 1, 1477, 768, 24, 1636, 198, 186,...     0   \n",
       "cv696_29619.txt  [1823, 4, 7043, 1201, 18, 32, 151, 12, 1, 15, ...     0   \n",
       "cv697_12106.txt  [384, 21, 387, 1028, 38622, 3, 196, 9049, 353,...     0   \n",
       "cv698_16930.txt  [25, 4, 106, 617, 894, 107, 6, 2451, 1155, 393...     0   \n",
       "cv699_7773.txt   [1, 646, 4, 14, 28, 6, 83, 262, 10825, 536, 84...     0   \n",
       "\n",
       "                 number of unique words  review_length  \n",
       "cv000_29590.txt                     411            679  \n",
       "cv001_18431.txt                     311            650  \n",
       "cv002_15918.txt                     243            416  \n",
       "cv003_11664.txt                     496            997  \n",
       "cv004_11636.txt                     341            644  \n",
       "...                                 ...            ...  \n",
       "cv695_22268.txt                     328            594  \n",
       "cv696_29619.txt                     470            751  \n",
       "cv697_12106.txt                     355            628  \n",
       "cv698_16930.txt                     343            652  \n",
       "cv699_7773.txt                      299            604  \n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c235875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   66,  3436,    31, ...,     0,     0,     0],\n",
       "       [  149,   141,     3, ...,     0,     0,     0],\n",
       "       [  715,   287,  4181, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [32769,    11,     1, ...,     0,     0,     0],\n",
       "       [    3,   141,     1, ...,     0,     0,     0],\n",
       "       [ 3538,   192,   455, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f3622",
   "metadata": {},
   "source": [
    "# (c) Word Embeddings\n",
    "\n",
    "## i. One can use tokenized text as inputs to a deep neural network. However, a recent breakthrough in NLP suggests that more sophisticated representations of text yield better results. \n",
    "## These sophisticated representations are called word embeddings. “Word embedding is a term used for representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.”4. Most deep learning modules (including Keras) provide a convenient way to convert positive integer representations of words into a word embedding by an “Embedding layer.” The layer accepts arguments that define the mapping of words into embeddings, including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value). The layer also allows you to specify the dimension for each word vector, called the “output dimension.” \n",
    "\n",
    "## We would like to use a word embedding layer for this project. Assume that we are interested in the top 5,000 words. This means that in each integer sequence that represents each document, we set to zero those integers that represent words that are not among the top 5,000 words in the document.5 \n",
    "If you feel more adventurous, use all the words that appear in this corpus. Choose the length of the embedding vector for each word to be 32. Hence, each document is represented as a 32 × 500 matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b58127b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f0d2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.where(tt > 5000, 0, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f09fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = pd.DataFrame(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e3e231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "570532e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 737, 32)           160032    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,032\n",
      "Trainable params: 160,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Embedding(5001, 32, input_length = int(L)))\n",
    "model.compile('rmsprop', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48ac93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(tt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d78ed94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.02855923,  0.0262524 , -0.04226223, ..., -0.03453847,\n",
       "          0.03241756, -0.01394301],\n",
       "        [ 0.0364988 ,  0.01446814, -0.03312489, ...,  0.01293173,\n",
       "         -0.02582396, -0.02356063],\n",
       "        [ 0.01260618, -0.0079837 ,  0.03102135, ..., -0.01577402,\n",
       "          0.04753652,  0.03136572],\n",
       "        ...,\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931]],\n",
       "\n",
       "       [[ 0.01369215,  0.03450667, -0.03989429, ..., -0.01602808,\n",
       "          0.04565407, -0.01769295],\n",
       "        [-0.01617713,  0.04931674,  0.04405883, ...,  0.03232816,\n",
       "          0.03992692,  0.04671763],\n",
       "        [ 0.01539748, -0.02948974,  0.04318077, ..., -0.00331772,\n",
       "         -0.01753768, -0.01292195],\n",
       "        ...,\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931]],\n",
       "\n",
       "       [[ 0.02472304,  0.00361764, -0.00991054, ..., -0.03527933,\n",
       "          0.0346188 ,  0.02455704],\n",
       "        [ 0.02804497, -0.0079926 ,  0.02722151, ...,  0.03542906,\n",
       "         -0.03265651,  0.04927501],\n",
       "        [ 0.04325822, -0.0196379 , -0.04241249, ...,  0.03266991,\n",
       "          0.00552144, -0.04624896],\n",
       "        ...,\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.00134622,  0.04106987,  0.02352989, ...,  0.04809869,\n",
       "          0.00773384,  0.00482583],\n",
       "        [-0.02756752,  0.02605275, -0.02959068, ...,  0.00254775,\n",
       "         -0.00805042,  0.03480906],\n",
       "        ...,\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931]],\n",
       "\n",
       "       [[ 0.01539748, -0.02948974,  0.04318077, ..., -0.00331772,\n",
       "         -0.01753768, -0.01292195],\n",
       "        [-0.01617713,  0.04931674,  0.04405883, ...,  0.03232816,\n",
       "          0.03992692,  0.04671763],\n",
       "        [-0.02756752,  0.02605275, -0.02959068, ...,  0.00254775,\n",
       "         -0.00805042,  0.03480906],\n",
       "        ...,\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931]],\n",
       "\n",
       "       [[ 0.03229428, -0.01792208, -0.0361176 , ...,  0.0322015 ,\n",
       "          0.02708724, -0.03844553],\n",
       "        [-0.01329452,  0.00284136, -0.03509521, ...,  0.02589468,\n",
       "          0.02664329, -0.04721062],\n",
       "        [ 0.02086211, -0.02087867, -0.02037327, ...,  0.00611049,\n",
       "         -0.04630215, -0.03940707],\n",
       "        ...,\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931],\n",
       "        [-0.04918926, -0.00057494,  0.03378801, ...,  0.03782611,\n",
       "         -0.00438098,  0.00186931]]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe76e3",
   "metadata": {},
   "source": [
    "## ii. Flatten the matrix of each document to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b626fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 737, 32, 32)       160032    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 754688)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,064\n",
      "Trainable params: 320,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Embedding(5001, 32, input_length = int(L)))\n",
    "model.add(Flatten())\n",
    "model.compile('rmsprop', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89f6e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = model.predict(tt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ead9acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03345566, -0.04930478,  0.02195961, ..., -0.02780213,\n",
       "        -0.03213926,  0.02080989],\n",
       "       [-0.03345566, -0.04930478,  0.02195961, ..., -0.02780213,\n",
       "        -0.03213926,  0.02080989],\n",
       "       [-0.03345566, -0.04930478,  0.02195961, ..., -0.02780213,\n",
       "        -0.03213926,  0.02080989],\n",
       "       ...,\n",
       "       [-0.03345566, -0.04930478,  0.02195961, ..., -0.02780213,\n",
       "        -0.03213926,  0.02080989],\n",
       "       [-0.03345566, -0.04930478,  0.02195961, ..., -0.02780213,\n",
       "        -0.03213926,  0.02080989],\n",
       "       [-0.03345566, -0.04930478,  0.02195961, ..., -0.02780213,\n",
       "        -0.03213926,  0.02080989]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78ce3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1['class'] = df3['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9988ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([tt1[0:700], tt1[1000:1700]])\n",
    "test = pd.concat([tt1[700:1000], tt1[1700:2000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa9582fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(columns = 'class')\n",
    "y_train = train['class']\n",
    "x_test = test.drop(columns = 'class')\n",
    "y_test = test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f231a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "085a70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99581102",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18e2fe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      int64\n",
       "1      int64\n",
       "2      int64\n",
       "3      int64\n",
       "4      int64\n",
       "       ...  \n",
       "732    int64\n",
       "733    int64\n",
       "734    int64\n",
       "735    int64\n",
       "736    int64\n",
       "Length: 737, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "881a242f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74306563",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2aa569cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec866a1a",
   "metadata": {},
   "source": [
    "# (d) Multi-Layer Perceptron\n",
    "\n",
    "## i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. Use a dropout rate of 20% for the first layer and 50% for the other layers. Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cff93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69581b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 23584)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                1179250   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,344,433\n",
      "Trainable params: 1,344,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5001, 32, input_length = int(L)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(layers.Dense(50, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(50, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(50, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a95fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',  metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d294952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 5ms/step - loss: 0.6993 - accuracy: 0.4957\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6598 - accuracy: 0.5950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb7e974ac0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 2, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d5512",
   "metadata": {},
   "source": [
    "## ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23d196a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8178571462631226"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose = 3)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33d6a09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766666531562805"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose = 3)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a431561",
   "metadata": {},
   "source": [
    "### The accuracy for train set is about 0.6642857193946838 and for test set is about 0.5083333253860474"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac11457",
   "metadata": {},
   "source": [
    "# (e) One-Dimensional Convolutional Neural Network:\n",
    "# Although CNNs are mainly used for image data, they can also be applied to text data, as text also has adjacency information. Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624aaf6",
   "metadata": {},
   "source": [
    "## i. After the embedding layer, insert a Conv1D layer. This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded word representations 3 vector elements of the word embedding at a time. The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "df5531a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 735, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 368, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 11776)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                588850    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 757,137\n",
      "Trainable params: 757,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5001, 32, input_length = int(L)))\n",
    "model.add(layers.Conv1D(32, 3))\n",
    "model.add(layers.MaxPooling1D(pool_size = 1, strides = 2))\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(50, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(50, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(50, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9775a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',  metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "721aa218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 7ms/step - loss: 0.6952 - accuracy: 0.5236\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.6747 - accuracy: 0.5807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb7c366d00>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 2, batch_size = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587bacdb",
   "metadata": {},
   "source": [
    "## ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c13f82c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7507143020629883"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose = 3)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4d937f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5616666674613953"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose = 3)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eda49a",
   "metadata": {},
   "source": [
    "### The accuracy for train set is about 0.8007143139839172 and for test set is about 0.5416666865348816"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b8f60",
   "metadata": {},
   "source": [
    "# (f) Long Short-Term Memory Recurrent Neural Network:\n",
    "# The structure of the LSTM we are going to use is shown in the following figure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e0496",
   "metadata": {},
   "source": [
    "## i. Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b68ed353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177,057\n",
      "Trainable params: 177,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5001, 32, input_length = int(L)))\n",
    "model.add(LSTM(units = 32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d106fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',  metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8532b6d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "140/140 [==============================] - 18s 119ms/step - loss: 0.6897 - accuracy: 0.5221\n",
      "Epoch 2/20\n",
      "140/140 [==============================] - 17s 120ms/step - loss: 0.6754 - accuracy: 0.5721\n",
      "Epoch 3/20\n",
      "140/140 [==============================] - 17s 123ms/step - loss: 0.6152 - accuracy: 0.6357\n",
      "Epoch 4/20\n",
      "140/140 [==============================] - 18s 129ms/step - loss: 0.5736 - accuracy: 0.6643\n",
      "Epoch 5/20\n",
      "140/140 [==============================] - 17s 121ms/step - loss: 0.5344 - accuracy: 0.6621\n",
      "Epoch 6/20\n",
      "140/140 [==============================] - 18s 130ms/step - loss: 0.4978 - accuracy: 0.6800\n",
      "Epoch 7/20\n",
      "140/140 [==============================] - 19s 136ms/step - loss: 0.4914 - accuracy: 0.6786\n",
      "Epoch 8/20\n",
      "140/140 [==============================] - 19s 137ms/step - loss: 0.4882 - accuracy: 0.6686\n",
      "Epoch 9/20\n",
      "140/140 [==============================] - 18s 129ms/step - loss: 0.4834 - accuracy: 0.6879\n",
      "Epoch 10/20\n",
      "140/140 [==============================] - 18s 125ms/step - loss: 0.4772 - accuracy: 0.6800\n",
      "Epoch 11/20\n",
      "140/140 [==============================] - 18s 128ms/step - loss: 0.4808 - accuracy: 0.6800\n",
      "Epoch 12/20\n",
      "140/140 [==============================] - 19s 135ms/step - loss: 0.4756 - accuracy: 0.6829\n",
      "Epoch 13/20\n",
      "140/140 [==============================] - 19s 134ms/step - loss: 0.4703 - accuracy: 0.6864\n",
      "Epoch 14/20\n",
      "140/140 [==============================] - 18s 131ms/step - loss: 0.4700 - accuracy: 0.6857\n",
      "Epoch 15/20\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 0.4684 - accuracy: 0.6857\n",
      "Epoch 16/20\n",
      "140/140 [==============================] - 18s 129ms/step - loss: 0.4615 - accuracy: 0.6850\n",
      "Epoch 17/20\n",
      "140/140 [==============================] - 19s 138ms/step - loss: 0.4652 - accuracy: 0.6850\n",
      "Epoch 18/20\n",
      "140/140 [==============================] - 19s 133ms/step - loss: 0.4951 - accuracy: 0.6786\n",
      "Epoch 19/20\n",
      "140/140 [==============================] - 18s 128ms/step - loss: 0.4881 - accuracy: 0.6743\n",
      "Epoch 20/20\n",
      "140/140 [==============================] - 18s 126ms/step - loss: 0.4735 - accuracy: 0.6707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb7c86d6d0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 20, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5483101",
   "metadata": {},
   "source": [
    "## ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ab30bdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6800000071525574"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose = 3)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98c97b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5583333373069763"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose = 3)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28858ae4",
   "metadata": {},
   "source": [
    "### The accuracy for train set is about 0.683571457862854 and for test set is about 0.5733333230018616"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccdf4cc",
   "metadata": {},
   "source": [
    "\n",
    "# Reference:\n",
    "\n",
    "## https://stackoverflow.com/questions/3159155/how-to-remove-all-integer-values-from-a-list-in-python\n",
    "\n",
    "## https://www.geeksforgeeks.org/how-to-read-multiple-text-files-from-folder-in-python/\n",
    "\n",
    "## https://www.delftstack.com/howto/python/python-remove-punctuation-from-list/#:~:text=The%20fastest%20and%20the%20most,translate()%20function.\n",
    "\n",
    "## unique count\n",
    "## https://towardsdatascience.com/dealing-with-list-values-in-pandas-dataframes-a177e534f173\n",
    "\n",
    "## https://www.geeksforgeeks.org/python-count-occurrences-of-each-word-in-given-text-file-using-dictionary/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
